# üß™ MindAware - Complete Testing Guide

## Quick Start

### Run All Tests (Recommended)
```bash
python run_tests.py
```

This runs all test suites and provides a comprehensive summary.

---

## Test Suites Overview

### 1Ô∏è‚É£  **Policy Tests** (`tests/test_policy.py`)
Tests the binary all-or-nothing decision logic.

**What it tests:**
- ‚úÖ ALL GOOD state detection (focus ‚â•0.6, negatives ‚â§0.4)
- üî¥ ALL BAD state detection (focus ‚â§0.4, negatives ‚â•0.6)
- üîÑ MIXED state detection (some good, some bad)
- Threshold boundary conditions
- Single parameter violations

**Run individually:**
```bash
python tests/test_policy.py
```

**Expected output:**
```
============================================================
TESTING: CognitivePolicy (Binary Logic)
============================================================

‚úÖ test_all_good_state PASSED
‚úÖ test_all_bad_state PASSED
‚úÖ test_mixed_state PASSED
‚úÖ test_threshold_boundaries PASSED
‚úÖ test_single_parameter_off PASSED

============================================================
‚úÖ ALL POLICY TESTS PASSED!
============================================================
```

---

### 2Ô∏è‚É£  **Tools Tests** (`tests/test_tools.py`)
Tests the three binary drone control functions.

**What it tests:**
- `takeoff()` - Goes to exactly 1.0m
- `land()` - Goes to exactly 0.0m
- `turn_around()` - Rotates exactly 180¬∞
- Tool dispatcher (`execute_tool`)
- Action history logging
- Binary altitude constraints (no gradual movement)

**Run individually:**
```bash
python tests/test_tools.py
```

**Expected output:**
```
============================================================
TESTING: DroneTools (Binary Controls)
============================================================

‚úÖ test_takeoff PASSED
‚úÖ test_land PASSED
‚úÖ test_turn_around PASSED
‚úÖ test_execute_tool PASSED
‚úÖ test_action_history PASSED
‚úÖ test_binary_altitude_constraint PASSED

============================================================
‚úÖ ALL TOOL TESTS PASSED!
============================================================
```

---

### 3Ô∏è‚É£  **Integration Tests** (`tests/test_integration.py`)
Tests complete end-to-end flows through the system.

**What it tests:**
- Complete flow: state ‚Üí policy ‚Üí tools ‚Üí result
- ALL GOOD scenario (takeoff to 1m)
- ALL BAD scenario (land to 0m)
- MIXED scenario (rotate 180¬∞)
- State transitions (good ‚Üí bad)
- Memory tracking across states

**Run individually:**
```bash
python tests/test_integration.py
```

**Expected output:**
```
============================================================
TESTING: Integration (End-to-End Flows)
============================================================

‚úÖ End-to-end ALL GOOD test PASSED
‚úÖ End-to-end ALL BAD test PASSED
‚úÖ End-to-end MIXED test PASSED
‚úÖ State transition test PASSED
‚úÖ Memory tracking test PASSED

============================================================
‚úÖ ALL INTEGRATION TESTS PASSED!
============================================================
```

---

### 4Ô∏è‚É£  **Simulator Tests** (`tests/test_simulators.py`)
Tests EEG and Drone simulators.

**What it tests:**
- EEG simulator scenarios:
  - `normal` - Generates all good parameters
  - `critical` - Generates all bad parameters
  - `mixed` - Generates mixed parameters
  - `degrading` - Gradual decline over time
- Drone simulator:
  - Altitude updates
  - Rotation updates
  - Battery drain
  - 1.0m altitude cap

**Run individually:**
```bash
python tests/test_simulators.py
```

**Expected output:**
```
============================================================
TESTING: Simulators (EEG & Drone)
============================================================

‚úÖ EEG Simulator - Normal PASSED
‚úÖ EEG Simulator - Critical PASSED
‚úÖ EEG Simulator - Mixed PASSED
‚úÖ EEG Simulator - Degrading PASSED
‚úÖ Drone Simulator PASSED
‚úÖ Drone Simulator - Battery drain PASSED

============================================================
‚úÖ ALL SIMULATOR TESTS PASSED!
============================================================
```

---

### 5Ô∏è‚É£  **API Tests** (`tests/test_api.py`)
Tests backend API endpoints (requires server running).

**Prerequisites:**
```bash
# In separate terminal:
cd src/api
uvicorn server:app --host 0.0.0.0 --port 8000 --reload
```

**What it tests:**
- `GET /health` - Health check
- `GET /` - Root status
- `POST /eeg/ingest` - EEG data ingestion
- `GET /eeg/status` - Adapter status
- `GET /eeg/state` - Current cognitive state
- `GET /logs` - Decision logs
- `GET /memory` - Agent memory

**Run individually:**
```bash
python tests/test_api.py
```

**Expected output:**
```
============================================================
TESTING: API Endpoints
============================================================

‚úÖ /health endpoint PASSED
‚úÖ / endpoint PASSED
‚úÖ /eeg/ingest endpoint PASSED
‚úÖ /eeg/status endpoint PASSED
‚úÖ /eeg/state endpoint PASSED
‚úÖ /logs endpoint PASSED
‚úÖ /memory endpoint PASSED

============================================================
‚úÖ ALL API TESTS PASSED!
============================================================
```

---

## Complete Testing Workflow

### Option 1: Quick Test (Unit + Integration)
```bash
# No backend needed
python run_tests.py
```

This runs tests 1-4 (skips API tests if backend not running).

### Option 2: Full Test Suite (Including API)
```bash
# Terminal 1: Start backend
cd src/api
uvicorn server:app --host 0.0.0.0 --port 8000 --reload

# Terminal 2: Run all tests
python run_tests.py
```

This runs ALL tests including API endpoints.

---

## Manual Testing Scenarios

### Test 1: Normal State (ALL GOOD)
```bash
python src/main.py --scenario normal --iterations 5 --interval 2
```

**Expected behavior:**
- Policy detects ALL GOOD
- Recommends `takeoff`
- Drone altitude goes to 1.0m
- All metrics show ‚úÖ

### Test 2: Critical State (ALL BAD)
```bash
python src/main.py --scenario critical --iterations 5 --interval 2
```

**Expected behavior:**
- Policy detects ALL BAD
- Recommends `land`
- Drone altitude goes to 0.0m
- All metrics show ‚ùå

### Test 3: Mixed State
```bash
python src/main.py --scenario mixed --iterations 5 --interval 2
```

**Expected behavior:**
- Policy detects MIXED
- Recommends `turn_around`
- Drone rotates 180¬∞
- Some metrics ‚úÖ, some ‚ùå

### Test 4: State Transitions
```bash
python src/main.py --scenario degrading --iterations 20 --interval 2
```

**Expected behavior:**
- Starts ALL GOOD (takeoff to 1m)
- Gradually degrades
- Eventually ALL BAD (land to 0m)
- Watch altitude drop: 1m ‚Üí 0m

---

## Testing with UI

### Test Setup
```bash
# Terminal 1: Backend
cd src/api && uvicorn server:app --reload

# Terminal 2: Frontend
cd ui/frontend && npm run dev

# Terminal 3: Simulation
python src/main.py --scenario degrading --iterations 30 --interval 2
```

### Visual Tests
1. **Open**: http://localhost:5173
2. **Watch for:**
   - Binary status badge changes (GREEN ‚Üí YELLOW ‚Üí RED)
   - Altitude bar moves (1m ‚Üí 0m)
   - Rotation compass spins (180¬∞ turns)
   - Threshold markers on metrics
   - Real-time chart updates

---

## CI/CD Integration

### GitHub Actions Example
```yaml
name: Test Suite

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - run: python run_tests.py
```

---

## Troubleshooting

### Test Failures

#### "Module not found" error
```bash
# Install dependencies
pip install -r requirements.txt
```

#### "Connection refused" (API tests)
```bash
# Start backend server
cd src/api
uvicorn server:app --host 0.0.0.0 --port 8000 --reload
```

#### "Import error" for src modules
```bash
# Run from project root
cd /Users/zain/Desktop/MindAware
python run_tests.py
```

### Debugging Individual Tests

**Add verbose output:**
```python
# In any test file, add before test:
print(f"DEBUG: variable_name = {variable_name}")
```

**Run with Python debugger:**
```bash
python -m pdb tests/test_policy.py
```

---

## Test Coverage

### Current Coverage

| Component | Tests | Coverage |
|-----------|-------|----------|
| Policy Engine | 5 tests | 100% |
| Drone Tools | 6 tests | 100% |
| Integration | 5 tests | ~90% |
| Simulators | 6 tests | ~85% |
| API Endpoints | 7 tests | ~80% |

### What's Tested

‚úÖ Binary decision logic (all good/bad/mixed)  
‚úÖ Threshold boundaries (0.4 and 0.6)  
‚úÖ Tool execution (takeoff/land/turn_around)  
‚úÖ State transitions  
‚úÖ Memory tracking  
‚úÖ Simulator scenarios  
‚úÖ API endpoints  
‚úÖ End-to-end flows  

### What's NOT Tested (Future)

‚è≠Ô∏è LLM integration (requires API key)  
‚è≠Ô∏è WebSocket connections  
‚è≠Ô∏è Real EEG hardware integration  
‚è≠Ô∏è Frontend component tests  
‚è≠Ô∏è Load/stress testing  
‚è≠Ô∏è Multi-agent scenarios  

---

## Performance Benchmarks

### Expected Test Times

| Test Suite | Time | Notes |
|------------|------|-------|
| Policy | ~0.1s | Fast unit tests |
| Tools | ~0.1s | Fast unit tests |
| Integration | ~0.2s | Multiple components |
| Simulators | ~0.5s | Iterative scenarios |
| API | ~2s | Network calls |
| **Total** | **~3s** | Full suite |

---

## Test Data

### Sample Cognitive States

**All Good:**
```json
{
  "focus": 0.8,
  "fatigue": 0.2,
  "overload": 0.3,
  "stress": 0.2
}
```

**All Bad:**
```json
{
  "focus": 0.3,
  "fatigue": 0.7,
  "overload": 0.8,
  "stress": 0.9
}
```

**Mixed:**
```json
{
  "focus": 0.5,
  "fatigue": 0.5,
  "overload": 0.7,
  "stress": 0.3
}
```

---

## Continuous Testing

### Watch Mode (Development)
```bash
# Install watchdog
pip install watchdog

# Run tests on file change
watchmedo shell-command \
  --patterns="*.py" \
  --recursive \
  --command='python run_tests.py' \
  src/
```

### Pre-commit Hook
```bash
# .git/hooks/pre-commit
#!/bin/bash
python run_tests.py
if [ $? -ne 0 ]; then
  echo "Tests failed. Commit aborted."
  exit 1
fi
```

---

## Test Report Generation

### Generate HTML Report
```bash
# Install pytest-html
pip install pytest pytest-html

# Run with report
pytest tests/ --html=test_report.html
```

---

## Quick Reference

### Run specific test
```bash
python tests/test_policy.py          # Policy only
python tests/test_tools.py           # Tools only
python tests/test_integration.py     # Integration only
python tests/test_simulators.py      # Simulators only
python tests/test_api.py             # API only (backend required)
```

### Run all tests
```bash
python run_tests.py                  # Master runner
```

### Test with verbose output
```bash
python -v run_tests.py               # Verbose mode
```

---

## Success Criteria

### ‚úÖ All Tests Pass When:
- Policy correctly detects all 3 states
- Tools execute binary actions correctly
- Integration flows complete successfully
- Simulators generate valid data
- API endpoints respond correctly

### ‚ö†Ô∏è Tests May Fail If:
- Dependencies not installed (`pip install -r requirements.txt`)
- Backend not running (for API tests)
- Python version < 3.11
- Port 8000 already in use

---

**Built with ‚ù§Ô∏è for reliable, testable cognitive monitoring**


